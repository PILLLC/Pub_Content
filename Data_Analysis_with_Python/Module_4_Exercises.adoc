== Module {module-number}: Web Scraping

[grid=none,frame=none,cols="25%a,75%a"]
|===
|image::agenda.svg[align="left",pdfwidth=50%]|Upon completion of these exercises, students should be able to:

* Create a Scrapy spider to scrape product information from an e-commerce website. _(Exercise {module-number}A)_ 
* Use Selenium to scrape data from a dynamically loaded webpage with JavaScript content. _(Exercise {module-number}B)_ 
* Implement HTML parsing and scraping with BeautifulSoup to extract information from a news article webpage. _(Exercise {module-number}C)_ 
* Combine Scrapy, Selenium, and BeautifulSoup to scrape data from a website with both structured and dynamic content. _(Exercise {module-number}D)_ 
|===

<<<

=== Exercise {module-number}A - Create a Scrapy spider to scrape product information from an e-commerce website.

Below is an example of a Scrapy spider that can be used to scrape product information from an e-commerce website:

[source,python]
----
import scrapy

class ECommerceSpider(scrapy.Spider):
    name = 'ecommerce'
    start_urls = ['https://amazon.com/products']

    def parse(self, response):
        # Extract product URLs
        product_urls = response.css('a.product-link::attr(href)').extract()
        
        # Follow each product URL and parse product details
        for product_url in product_urls:
            yield scrapy.Request(url=product_url, callback=self.parse_product)
            
        # Follow pagination links
        next_page = response.css('a.next-page::attr(href)').extract_first()
        if next_page:
            yield scrapy.Request(url=next_page, callback=self.parse)

    def parse_product(self, response):
        # Extract product details
        product = {
            'name': response.css('h1.product-name::text').extract_first(),
            'price': response.css('span.product-price::text').extract_first(),
            'description': response.css('div.product-description::text').extract_first(),
            # Add more fields as needed
        }
        
        yield product
----

In this example:
- The spider starts by visiting the URL specified in the `start_urls` list.
- It extracts product URLs from the initial page and follows each URL to parse the details of individual products.
- It also follows pagination links to scrape products from multiple pages.
- The `parse_product` method is responsible for extracting details such as product name, price, description, etc., from each product page.

You can customize this spider according to the specific structure and behavior of the e-commerce website you want to scrape. Make sure to inspect the HTML structure of the website to determine the appropriate CSS selectors or XPaths for extracting data. Additionally, be respectful of the website's terms of service and robots.txt file when scraping data.

<<<

=== Exercise {module-number}B - Use Selenium to scrape data from a dynamically loaded webpage with JavaScript content.
Here's an example of how you can use Selenium to scrape data from a dynamically loaded webpage with JavaScript content:

[source,python]
----
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Start a Selenium webdriver
driver = webdriver.Chrome()

# Navigate to the webpage with dynamically loaded content
driver.get("https://example.com")

# Wait for the dynamically loaded content to appear
wait = WebDriverWait(driver, 10)
element = wait.until(EC.presence_of_element_located((By.CLASS_NAME, "dynamic-content")))

# Extract data from the dynamically loaded content
data = element.text

# Print or process the extracted data as needed
print(data)

# Close the webdriver
driver.quit()
----

In this example:
- We start a Selenium webdriver, in this case, using the Chrome webdriver.
- We navigate to the webpage with dynamically loaded content using `driver.get()`.
- We use WebDriverWait to wait for the dynamically loaded content to appear. Here, we wait for an element with class name "dynamic-content" to be present.
- Once the dynamically loaded content appears, we extract its text using the `text` attribute of the WebElement object.
- Finally, we print or process the extracted data as needed, and then close the webdriver using `driver.quit()`.

You may need to adjust the locator strategy (e.g., class name, CSS selector, XPath) used in `WebDriverWait` according to the specific structure of the webpage you are scraping.

<<<

=== Exercise {module-number}C - Implement HTML parsing and scraping with BeautifulSoup to extract information from a news article webpage.

Here's an example of how you can use BeautifulSoup to scrape information from a news article webpage:

[source,python]
----
import requests
from bs4 import BeautifulSoup

# URL of the news article webpage
url = "https://www.example.com/news/article"

# Send a GET request to the webpage
response = requests.get(url)

# Check if the request was successful (status code 200)
if response.status_code == 200:
    # Parse the HTML content of the webpage using BeautifulSoup
    soup = BeautifulSoup(response.content, "html.parser")
    
    # Extract the title of the article
    title = soup.find("h1").text.strip()
    print("Title:", title)
    
    # Extract the publication date of the article
    date = soup.find("time").text.strip()
    print("Publication Date:", date)
    
    # Extract the author(s) of the article
    author = soup.find("div", class_="author").text.strip()
    print("Author:", author)
    
    # Extract the main content of the article
    content = soup.find("div", class_="article-content").text.strip()
    print("Content:", content)
    
    # Extract other relevant information as needed
    
else:
    print("Failed to retrieve the webpage.")
----

In this example:
- We send a GET request to the URL of the news article webpage using the `requests.get()` function.
- If the request is successful (status code 200), we parse the HTML content of the webpage using BeautifulSoup.
- We then use various methods provided by BeautifulSoup, such as `find()` or `find_all()`, to locate specific elements in the HTML document, such as the title, publication date, author, and main content of the article.
- We extract the text content of these elements using the `text` attribute and strip any leading or trailing whitespace using the `strip()` method.
- Finally, we print or process the extracted information as needed.

<<<

=== Exercise {module-number}D - Combine Scrapy, Selenium, and BeautifulSoup to scrape data from a website with both structured and dynamic content.

Combining Scrapy, Selenium, and BeautifulSoup can be quite powerful for scraping websites with both structured and dynamic content. Here's a general approach to achieve this:

1. **Use Scrapy for web crawling**: Scrapy is great for crawling websites and extracting structured data from multiple pages. You can define the structure of the website, including the URLs to visit and the data to extract from each page.

2. **Use Selenium for interacting with dynamic content**: Selenium can be used to interact with JavaScript-driven content, such as clicking buttons, filling out forms, or scrolling through pages. This is useful for scraping data from websites that heavily rely on JavaScript to render content dynamically.

3. **Use BeautifulSoup for parsing HTML content**: Once Selenium has loaded the dynamic content, you can pass the HTML source to BeautifulSoup for parsing. BeautifulSoup provides simple and intuitive methods for extracting data from HTML documents.

Here's a basic example of how you can combine these tools:

[source,python]
----
import scrapy
from scrapy.selector import Selector
from scrapy_selenium import SeleniumRequest
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

class MySpider(scrapy.Spider):
    name = "my_spider"

    def start_requests(self):
        # Start with SeleniumRequest to load the initial page
        yield SeleniumRequest(url="https://example.com", callback=self.parse)

    def parse(self, response):
        # Use Selenium to interact with dynamic content
        # For example, clicking a button to load more content
        driver = response.meta['driver']
        load_more_button = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, "//button[@id='load-more-button']"))
        )
        load_more_button.click()

        # Wait for dynamic content to load
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, "//div[@class='dynamic-content']"))
        )

        # Pass the HTML source to BeautifulSoup for parsing
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # Extract structured data using Scrapy Selectors
        # For example, extract product titles
        titles = Selector(text=driver.page_source).xpath("//h2[@class='product-title']/text()").extract()

        # Extract dynamic content using BeautifulSoup
        # For example, extract text from a div with class 'dynamic-content'
        dynamic_content = soup.find("div", class_="dynamic-content").get_text()

        # Process or yield the extracted data as needed
        yield {
            'titles': titles,
            'dynamic_content': dynamic_content
        }
----

In this example:
- We start by using `SeleniumRequest` to load the initial page with Selenium.
- Inside the `parse` method, we use Selenium to interact with dynamic content, such as clicking a button to load more content.
- Once the dynamic content is loaded, we pass the HTML source to BeautifulSoup for parsing.
- We use Scrapy Selectors to extract structured data (e.g., product titles) and BeautifulSoup to extract dynamic content (e.g., text from a div with a specific class).
- Finally, we process or yield the extracted data as needed.

<<<