== Module {module-number}: Time Series Analysis

[grid=none,frame=none,cols="25%a,75%a"]
|===
|image::agenda.svg[align="left",pdfwidth=50%]|Upon completion of these exercises, students should be able to:

* Explore and visualize the characteristics of time series data, including trend, seasonality, and noise. _(Exercise {module-number}A)_
* Use Darts to build time series forecasting models and evaluate their performance on historical data. _(Exercise {module-number}B)_
* Perform time series decomposition and anomaly detection using Kats on real-world datasets. _(Exercise {module-number}C)_
* Extract relevant features from time series data using Tsfresh and apply them to machine learning tasks._(Exercise {module-number}D)_
|===

<<<

=== Exercise {module-number}A - Explore and visualize the characteristics of time series data, including trend, seasonality, and noise.
Exploring and visualizing time series data can provide valuable insights into its characteristics, including trend, seasonality, and noise. Here's how you can do it using Python with libraries such as Pandas, Matplotlib, and Statsmodels:

1. **Load the Time Series Data**: Load your time series data into a Pandas DataFrame.

2. **Visualize the Time Series**: Plot the time series data to get an initial understanding of its overall pattern.

3. **Decompose the Time Series**: Use decomposition techniques to separate the time series into its constituent components: trend, seasonality, and residual (noise).

4. **Visualize Trend and Seasonality**: Plot the trend and seasonality components of the time series separately.

5. **Visualize Residuals (Noise)**: Plot the residual component of the time series to examine any remaining patterns or irregularities.

Here's an example implementation:

[source,python]
----
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# Step 1: Load the Time Series Data
# Assuming 'date' is the index and 'value' is the time series data
df = pd.read_csv('your_time_series_data.csv', parse_dates=['date'], index_col='date')

# Step 2: Visualize the Time Series
plt.figure(figsize=(10, 6))
plt.plot(df.index, df['value'], label='Original Time Series')
plt.title('Time Series Data')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.show()

# Step 3: Decompose the Time Series
decomposition = seasonal_decompose(df['value'], model='additive')

# Step 4: Visualize Trend and Seasonality
plt.figure(figsize=(10, 6))
plt.plot(df.index, decomposition.trend, label='Trend')
plt.title('Trend Component of Time Series')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(df.index, decomposition.seasonal, label='Seasonality')
plt.title('Seasonality Component of Time Series')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.show()

# Step 5: Visualize Residuals (Noise)
plt.figure(figsize=(10, 6))
plt.plot(df.index, decomposition.resid, label='Residuals (Noise)')
plt.title('Residual Component of Time Series')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.show()
----

In this example:
- We load the time series data into a Pandas DataFrame.
- We visualize the original time series data.
- We decompose the time series into its trend, seasonality, and residual components using the seasonal decomposition method from Statsmodels.
- We visualize the trend, seasonality, and residual components separately to better understand the characteristics of the time series data.

<<<

=== Exercise {module-number}B - Use Darts to build time series forecasting models and evaluate their performance on historical data.
Darts is a Python library designed for easy manipulation and forecasting of time series data. It provides various models and tools for time series forecasting. Here's a step-by-step guide on how to use Darts to build time series forecasting models and evaluate their performance on historical data:

1. **Install Darts**: If you haven't already installed Darts, you can do so using pip:
   
   pip install darts

2. **Load the Time Series Data**: Load your time series data into a Darts TimeSeries object.

3. **Split the Data**: Split the data into training and validation sets. Typically, you'll use a portion of the data for training and keep the rest for validation to evaluate the model's performance.

4. **Choose a Forecasting Model**: Choose a forecasting model from Darts, such as Exponential Smoothing, ARIMA, Prophet, or any other model that fits your data.

5. **Fit the Model**: Fit the chosen model to the training data.

6. **Make Forecasts**: Use the fitted model to make forecasts on the validation set.

7. **Evaluate Performance**: Evaluate the performance of the forecasts using appropriate metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), or Root Mean Squared Error (RMSE).

8. **Visualize Forecasts**: Visualize the forecasts along with the actual data to see how well the model captures the underlying patterns.

Here's a basic example demonstrating these steps:

[source,python]
----
from darts import TimeSeries
from darts.models import ExponentialSmoothing
from darts.metrics import mape
import matplotlib.pyplot as plt

# Step 2: Load the Time Series Data
# Assuming 'date' is the index and 'value' is the time series data
# Replace 'your_time_series_data.csv' with the path to your dataset
series = TimeSeries.from_csv('your_time_series_data.csv', parse_dates=True, index_col='date')

# Step 3: Split the Data
train, val = series.split_before(pd.Timestamp('2022-01-01'))

# Step 4: Choose a Forecasting Model and Fit the Model
model = ExponentialSmoothing()
model.fit(train)

# Step 6: Make Forecasts
forecast = model.predict(len(val))

# Step 7: Evaluate Performance
mape_error = mape(val, forecast)
print(f"MAPE: {mape_error:.2f}%")

# Step 8: Visualize Forecasts
plt.figure(figsize=(10, 6))
series.plot(label='Actual')
forecast.plot(label='Forecast', lw=2)
plt.title('Forecasting with Exponential Smoothing')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.show()
----

In this example:
- We load the time series data into a Darts TimeSeries object.
- We split the data into a training set and a validation set.
- We choose an Exponential Smoothing model and fit it to the training data.
- We make forecasts on the validation set.
- We evaluate the forecasts using Mean Absolute Percentage Error (MAPE).
- We visualize the actual data and the forecasts to assess the model's performance.

<<<

=== Exercise {module-number}C - Perform time series decomposition and anomaly detection using Kats on real-world datasets.
Kats is a powerful time series analysis toolkit developed by Facebook. It provides various functionalities for time series analysis, including time series decomposition and anomaly detection. Here's a step-by-step guide on how to perform time series decomposition and anomaly detection using Kats on real-world datasets:

1. **Install Kats**: If you haven't already installed Kats, you can do so using pip:

   pip install kats

2. **Load the Time Series Data**: Load your time series data into a pandas DataFrame or a Kats TimeSeriesData object.

3. **Perform Time Series Decomposition**: Use Kats to decompose the time series into its trend, seasonality, and residual components.

4. **Detect Anomalies**: Apply anomaly detection algorithms provided by Kats to detect anomalies in the time series.

5. **Visualize Results**: Visualize the original time series, its components after decomposition, and detected anomalies to gain insights into the data.

Here's a basic example demonstrating these steps:

[source,python]
----
from kats.consts import TimeSeriesData
from kats.detectors.trend_mk import MKDetector
from kats.detectors.outlier import OutlierDetector
from kats.detectors.seasonal_decomposition import TimeSeriesDecompositionDetector
import matplotlib.pyplot as plt

# Step 2: Load the Time Series Data
# Assuming 'date' is the index and 'value' is the time series data
# Replace 'your_time_series_data.csv' with the path to your dataset
# Load data into a Kats TimeSeriesData object
ts_data = TimeSeriesData.load_csv('your_time_series_data.csv')

# Step 3: Perform Time Series Decomposition
# Use seasonal decomposition to decompose the time series into trend, seasonality, and residual components
decomposition_detector = TimeSeriesDecompositionDetector(ts_data)
decomposition_result = decomposition_detector.run()

# Step 4: Detect Anomalies
# Use an anomaly detection algorithm, such as the Mann-Kendall trend change detector
mk_detector = MKDetector(ts_data)
mk_outliers = mk_detector.detector()

# Alternatively, you can use other outlier detection algorithms provided by Kats
# For example, you can use an outlier detector based on z-score
# outlier_detector = OutlierDetector(ts_data)
# zscore_outliers = outlier_detector.detector()

# Step 5: Visualize Results
# Plot the original time series, its decomposition components, and detected anomalies
plt.figure(figsize=(12, 8))

# Plot original time series
plt.subplot(3, 1, 1)
plt.plot(ts_data.time, ts_data.value)
plt.title('Original Time Series')

# Plot decomposed components
plt.subplot(3, 1, 2)
plt.plot(ts_data.time, decomposition_result.trend)
plt.plot(ts_data.time, decomposition_result.seasonal)
plt.plot(ts_data.time, decomposition_result.residue)
plt.title('Decomposed Components')

# Plot detected anomalies
plt.subplot(3, 1, 3)
plt.plot(ts_data.time, ts_data.value)
plt.scatter(mk_outliers['time'], mk_outliers['value'], color='red', label='Anomalies')
plt.title('Anomaly Detection')
plt.legend()

plt.tight_layout()
plt.show()
----

In this example:
- We load the time series data into a Kats TimeSeriesData object.
- We perform time series decomposition using Kats to decompose the time series into trend, seasonality, and residual components.
- We detect anomalies using the Mann-Kendall trend change detector provided by Kats.
- We visualize the original time series, its decomposition components, and detected anomalies to gain insights into the data.

<<<

=== Exercise {module-number}D - Extract relevant features from time series data using Tsfresh and apply them to machine learning tasks.
Tsfresh is a Python library that provides functionalities for feature extraction from time series data. It automatically extracts a large number of features from time series data, which can then be used for various machine learning tasks such as classification, regression, and clustering. Here's how you can use Tsfresh to extract relevant features from time series data and apply them to machine learning tasks:

1. **Install Tsfresh**: If you haven't already installed Tsfresh, you can do so using pip:
   ```
   pip install tsfresh
   ```

2. **Prepare Your Time Series Data**: Organize your time series data into a pandas DataFrame where each column represents a different time series, and each row represents a timestamp.

3. **Extract Features**: Use Tsfresh to extract features from your time series data. Tsfresh will automatically generate a large number of features based on the provided time series data.

4. **Preprocess Features**: Preprocess the extracted features as needed, such as handling missing values or scaling the features.

5. **Apply Machine Learning Models**: Use the extracted features as input to machine learning models for tasks such as classification, regression, or clustering.

Here's a basic example demonstrating these steps:

[source,python]
----
from tsfresh import extract_features
from tsfresh.utilities.dataframe_functions import impute
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd

# Step 2: Prepare Your Time Series Data
# Create a pandas DataFrame with time series data
# Each column represents a different time series
# Each row represents a timestamp
# Replace 'your_time_series_data.csv' with the path to your dataset
df = pd.read_csv('your_time_series_data.csv')

# Step 3: Extract Features
# Extract features from the time series data using Tsfresh
# Set the column 'id' to uniquely identify each time series
extracted_features = extract_features(df, column_id='id')

# Step 4: Preprocess Features
# Handle missing values in the extracted features
impute(extracted_features)

# Step 5: Apply Machine Learning Models
# Split the data into training and testing sets
X = extracted_features  # Features
y = df['target']         # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest classifier
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
----

In this example:
- We use Tsfresh to extract features from the time series data stored in a pandas DataFrame.
- We preprocess the extracted features to handle any missing values.
- We split the data into training and testing sets and train a Random Forest classifier using the extracted features.
- We evaluate the performance of the trained classifier on the test set using accuracy as the metric.

<<<